"""
Adaptive Search V2 - AI-Powered Search Query Optimization
S·ª≠ d·ª•ng AI ƒë·ªÉ ph√¢n t√≠ch keyword v√† t·∫°o search query ch√≠nh x√°c
"""

import json
import os
from datetime import datetime
from pathlib import Path
from google import genai
from google.genai import types


class AdaptiveSearchV2:
    """
    Adaptive Search v·ªõi AI-powered query optimization
    
    Kh√°c bi·ªát so v·ªõi v1:
    - AI ph√¢n t√≠ch keyword ƒë·ªÉ hi·ªÉu √Ω ƒë·ªãnh t√¨m ki·∫øm
    - T·ª± ƒë·ªông th√™m context ph√π h·ª£p (t√°c gi·∫£, nƒÉm, th·ªÉ lo·∫°i...)
    - H·ªçc t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm ƒë·ªÉ c·∫£i thi·ªán
    """
    
    def __init__(self, site_id, ai_client):
        self.site_id = site_id
        self.ai_client = ai_client
        self.profile_path = Path(f"profiles/{site_id}_profile.json")
        self.history_path = Path(f"profiles/{site_id}_history.json")
        
        # T·∫°o th∆∞ m·ª•c
        self.profile_path.parent.mkdir(exist_ok=True)
        
        # Load profile v√† history
        self.profile = self.load_profile()
        self.search_history = self.load_history()
    
    def load_profile(self):
        """Load profile t·ª´ file"""
        if self.profile_path.exists():
            with open(self.profile_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def save_profile(self):
        """L∆∞u profile"""
        with open(self.profile_path, 'w', encoding='utf-8') as f:
            json.dump(self.profile, f, ensure_ascii=False, indent=2)
    
    def load_history(self):
        """Load history"""
        if self.history_path.exists():
            with open(self.history_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def save_history(self):
        """L∆∞u history"""
        with open(self.history_path, 'w', encoding='utf-8') as f:
            json.dump(self.search_history, f, ensure_ascii=False, indent=2)
    
    def initialize_from_description(self, site_description):
        """T·∫°o profile t·ª´ m√¥ t·∫£ website"""
        
        prompt = f"""
T·∫°o search profile cho website:

M√î T·∫¢: {site_description}

Tr·∫£ v·ªÅ JSON:

{{
    "site_description": "{site_description}",
    "site_niche": "ch·ªß ƒë·ªÅ ch√≠nh",
    "content_focus": "review|news|tutorial|entertainment",
    "search_strategy": {{
        "default_context": "context m·∫∑c ƒë·ªãnh th√™m v√†o search",
        "domain_hints": ["domain ∆∞u ti√™n"],
        "avoid_terms": ["t·ª´ c·∫ßn tr√°nh trong k·∫øt qu·∫£"]
    }},
    "version": 1,
    "created_at": "{datetime.now().isoformat()}"
}}

Ch·ªâ tr·∫£ v·ªÅ JSON.
"""
        
        try:
            response = self.ai_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=1000,
                )
            )
            
            result_text = response.text.strip()
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0]
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0]
            
            self.profile = json.loads(result_text)
            self.save_profile()
            
            return self.profile
            
        except Exception as e:
            # Fallback
            self.profile = {
                "site_description": site_description,
                "site_niche": "general",
                "content_focus": "mixed",
                "search_strategy": {
                    "default_context": "",
                    "domain_hints": [],
                    "avoid_terms": ["mua", "b√°n", "gi√°"]
                },
                "version": 1,
                "created_at": datetime.now().isoformat(),
                "error": str(e)
            }
            self.save_profile()
            return self.profile
    
    def analyze_keyword_for_search(self, keyword, category_name=""):
        """
        AI ph√¢n t√≠ch keyword ƒë·ªÉ t·∫°o search query t·ªëi ∆∞u
        
        Args:
            keyword: T·ª´ kh√≥a g·ªëc
            category_name: Danh m·ª•c (optional)
        
        Returns:
            dict: Search strategy
        """
        
        site_context = ""
        if self.profile:
            site_context = f"Website niche: {self.profile.get('site_niche', 'general')}"
        
        analysis_prompt = f"""
Ph√¢n t√≠ch keyword ƒë·ªÉ t·∫°o Google search query t·ªëi ∆∞u.

{site_context}
CATEGORY: {category_name}
KEYWORD: {keyword}

Nhi·ªám v·ª•: X√°c ƒë·ªãnh keyword n√†y l√† g√¨ v√† c·∫ßn search th·∫ø n√†o ƒë·ªÉ t√¨m b√†i vi·∫øt g·ªëc ch·∫•t l∆∞·ª£ng.

Tr·∫£ v·ªÅ JSON:

{{
    "keyword_type": "author_name|book_title|character_name|topic|event",
    "search_intent": "find_bio|find_review|find_info|find_news",
    "optimal_query": "query t·ªëi ∆∞u ƒë·ªÉ search Google",
    "query_components": {{
        "base": "keyword g·ªëc",
        "context": "context th√™m v√†o (vd: ti·ªÉu thuy·∫øt, t√°c gi·∫£, review...)",
        "filters": "b·ªô l·ªçc (vd: site:domain, filetype:...)"
    }},
    "expected_sources": ["domain ho·∫∑c lo·∫°i ngu·ªìn mong ƒë·ª£i"],
    "avoid_sources": ["domain ho·∫∑c lo·∫°i ngu·ªìn c·∫ßn tr√°nh"]
}}

V√ç D·ª§:
- Keyword "Thi√™n T·∫±m Th·ªï ƒê·∫≠u" + Category "Review T√°c Gi·∫£"
  ‚Üí optimal_query: "Thi√™n T·∫±m Th·ªï ƒê·∫≠u t√°c gi·∫£ ti·ªÉu thuy·∫øt"
  ‚Üí expected_sources: ["wikipedia.org", "novelupdates.com"]

- Keyword "ƒê·∫•u Ph√° Th∆∞∆°ng Khung" + Category "Review Truy·ªán"
  ‚Üí optimal_query: "ƒê·∫•u Ph√° Th∆∞∆°ng Khung review truy·ªán"
  ‚Üí expected_sources: ["truyenfull.vn", "wikidich.com"]

Ch·ªâ tr·∫£ v·ªÅ JSON.
"""
        
        try:
            response = self.ai_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=analysis_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                    max_output_tokens=800,
                )
            )
            
            result_text = response.text.strip()
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0]
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0]
            
            strategy = json.loads(result_text)
            
            print(f"üìä Keyword type: {strategy.get('keyword_type', 'unknown')}")
            print(f"üîç Optimal query: {strategy.get('optimal_query', keyword)}")
            
            return strategy
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói ph√¢n t√≠ch keyword: {e}")
            # Fallback: search query ƒë∆°n gi·∫£n
            return {
                "keyword_type": "unknown",
                "search_intent": "find_info",
                "optimal_query": keyword,
                "query_components": {
                    "base": keyword,
                    "context": "",
                    "filters": ""
                },
                "expected_sources": [],
                "avoid_sources": []
            }
    
    def build_search_query(self, keyword, category_name=""):
        """
        T·∫°o search query t·ªëi ∆∞u
        
        Args:
            keyword: T·ª´ kh√≥a
            category_name: Danh m·ª•c
        
        Returns:
            str: Optimized search query
        """
        
        # Analyze keyword
        strategy = self.analyze_keyword_for_search(keyword, category_name)
        
        # Return optimal query
        return strategy.get('optimal_query', keyword)
    
    def score_search_result(self, result_url, result_title, result_snippet, strategy):
        """
        ƒê√°nh gi√° k·∫øt qu·∫£ t√¨m ki·∫øm
        
        Args:
            result_url: URL k·∫øt qu·∫£
            result_title: Ti√™u ƒë·ªÅ
            result_snippet: Snippet
            strategy: Search strategy t·ª´ analyze_keyword_for_search
        
        Returns:
            float: Score 0-1
        """
        
        score = 0.5  # Base score
        
        url_lower = result_url.lower()
        title_lower = result_title.lower()
        snippet_lower = result_snippet.lower()
        
        # Check expected sources
        expected_sources = strategy.get('expected_sources', [])
        if expected_sources:
            for source in expected_sources:
                if source.lower() in url_lower:
                    score += 0.3
                    break
        
        # Check avoid sources (penalty)
        avoid_sources = strategy.get('avoid_sources', [])
        if avoid_sources:
            for source in avoid_sources:
                if source.lower() in url_lower:
                    score -= 0.4
                    break
        
        # Check keyword in title (relevance)
        keyword_type = strategy.get('keyword_type', '')
        
        # Keyword ph·∫£i c√≥ trong title ho·∫∑c snippet
        query_base = strategy.get('query_components', {}).get('base', '').lower()
        if query_base:
            if query_base in title_lower:
                score += 0.2
            elif query_base in snippet_lower:
                score += 0.1
        
        return max(0, min(1, score))
    
    def learn_from_search(self, keyword, category_name, search_query, selected_url, selected_title):
        """
        H·ªçc t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
        
        Args:
            keyword: Keyword
            category_name: Category
            search_query: Query ƒë√£ d√πng
            selected_url: URL ƒë√£ ch·ªçn
            selected_title: Title c·ªßa URL ƒë√£ ch·ªçn
        """
        
        self.search_history.append({
            'keyword': keyword,
            'category': category_name,
            'search_query': search_query,
            'result_url': selected_url,
            'result_title': selected_title,
            'timestamp': datetime.now().isoformat()
        })
        
        self.save_history()
        
        # Auto refine sau 10 searches
        if len(self.search_history) % 10 == 0:
            return True
        
        return False
    
    def refine_profile(self):
        """
        C·∫£i thi·ªán profile d·ª±a tr√™n history
        """
        
        if len(self.search_history) < 5:
            return False
        
        recent = self.search_history[-20:]
        
        # Format history
        history_text = "\n".join([
            f"- Keyword: {h['keyword']} | Category: {h.get('category', 'N/A')} | Query: {h['search_query']}"
            for h in recent
        ])
        
        prompt = f"""
Ph√¢n t√≠ch search history v√† c·∫£i thi·ªán profile:

CURRENT PROFILE:
{json.dumps(self.profile, ensure_ascii=False, indent=2)}

RECENT SEARCHES:
{history_text}

D·ª±a v√†o patterns, ƒë·ªÅ xu·∫•t c·∫£i thi·ªán search_strategy:

{{
    "search_strategy": {{
        "default_context": "c·∫≠p nh·∫≠t context",
        "domain_hints": ["c·∫≠p nh·∫≠t domains"],
        "avoid_terms": ["c·∫≠p nh·∫≠t terms to avoid"]
    }},
    "version": {self.profile.get('version', 1) + 1},
    "last_refined": "{datetime.now().isoformat()}"
}}

Ch·ªâ tr·∫£ v·ªÅ JSON.
"""
        
        try:
            response = self.ai_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=1000,
                )
            )
            
            result_text = response.text.strip()
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0]
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0]
            
            improvements = json.loads(result_text)
            
            # Update profile
            self.profile['search_strategy'] = improvements['search_strategy']
            self.profile['version'] = improvements.get('version', self.profile.get('version', 1) + 1)
            self.profile['last_refined'] = improvements.get('last_refined', datetime.now().isoformat())
            
            self.save_profile()
            
            return True
            
        except Exception as e:
            print(f"Refine error: {e}")
            return False
    
    def get_stats(self):
        """L·∫•y stats"""
        return {
            'total_searches': len(self.search_history),
            'profile_version': self.profile.get('version', 0) if self.profile else 0,
            'site_niche': self.profile.get('site_niche', 'Unknown') if self.profile else 'Not initialized',
            'last_refined': self.profile.get('last_refined', 'Never') if self.profile else 'Never',
            'is_initialized': self.profile is not None
        }


# ============== HELPER FUNCTION ==============

def get_adaptive_search_v2(site_id, ai_client):
    """Get or create adaptive search v2 for a site"""
    return AdaptiveSearchV2(site_id, ai_client)


# ============== TEST ==============

if __name__ == "__main__":
    import sys
    from google import genai
    
    api_key = os.getenv("GEMINI_API_KEY")
    
    if not api_key:
        print("‚ùå Please set GEMINI_API_KEY environment variable")
        sys.exit(1)
    
    # Test
    print("=== Testing Adaptive Search V2 ===\n")
    
    client = genai.Client(api_key=api_key)
    search = AdaptiveSearchV2("test_site", client)
    
    # Initialize
    print("1. Initializing profile...")
    profile = search.initialize_from_description("Website review truy·ªán manga v√† ti·ªÉu thuy·∫øt")
    print(f"   ‚úÖ Niche: {profile['site_niche']}\n")
    
    # Test keyword analysis
    test_keywords = [
        ("Thi√™n T·∫±m Th·ªï ƒê·∫≠u", "Review T√°c Gi·∫£"),
        ("ƒê·∫•u Ph√° Th∆∞∆°ng Khung", "Review Truy·ªán"),
        ("m∆° th·∫•y r·∫Øn", "Gi·∫£i M√£ Gi·∫•c M∆°"),
    ]
    
    for keyword, category in test_keywords:
        print(f"2. Testing: {keyword} ({category})")
        query = search.build_search_query(keyword, category)
        print(f"   ‚Üí Optimized query: {query}\n")