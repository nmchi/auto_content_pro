import scrapy
import os
import requests
import trafilatura
from urllib.parse import urljoin, urlparse
from twisted.internet import error as twisted_error
from backend.items import BlogPostItem
from backend.adaptive_search import get_adaptive_system
from google import genai

class GoogleSpider(scrapy.Spider):
    name = "google_bot"
    
    # C·∫•u h√¨nh retry v√† timeout
    custom_settings = {
        'RETRY_TIMES': 3,
        'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
        'DOWNLOAD_TIMEOUT': 30,
        'ROBOTSTXT_OBEY': False,
        'COOKIES_ENABLED': True,
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    }

    def __init__(self, keyword=None, url=None, *args, **kwargs):
        super(GoogleSpider, self).__init__(*args, **kwargs)
        self.keyword = keyword
        self.direct_url = url

    def start_requests(self):
        """
        T√¨m ki·∫øm Google ho·∫∑c d√πng URL tr·ª±c ti·∫øp
        """
        # N·∫øu c√≥ URL tr·ª±c ti·∫øp th√¨ d√πng lu√¥n
        if self.direct_url:
            self.logger.info(f"S·ª≠ d·ª•ng URL tr·ª±c ti·∫øp: {self.direct_url}")
            yield scrapy.Request(
                url=self.direct_url, 
                callback=self.parse, 
                meta={'keyword': self.keyword or 'unknown'}
            )
            return
        
        # N·∫øu kh√¥ng c√≥ keyword th√¨ b√°o l·ªói
        if not self.keyword:
            self.logger.error("Thi·∫øu keyword! D√πng: -a keyword='t·ª´ kh√≥a'")
            return
        
        # L·∫•y API keys t·ª´ environment
        api_key = os.getenv("GOOGLE_API_KEY")
        cse_id = os.getenv("GOOGLE_CSE_ID")
        
        if not api_key or not cse_id:
            self.logger.error("Thi·∫øu GOOGLE_API_KEY ho·∫∑c GOOGLE_CSE_ID!")
            return
        
        # === ADAPTIVE SEARCH SYSTEM ===
        # Get site ID from WP URL
        wp_url = os.getenv("WP_URL", "")
        site_id = urlparse(wp_url).netloc.replace('.', '_') if wp_url else "default"
        
        # Initialize adaptive system
        try:
            ai_client = genai.Client(api_key=api_key)
            adaptive_system = get_adaptive_system(site_id, ai_client)
            
            # Check if profile exists
            if not adaptive_system.profile:
                self.logger.warning("‚ö†Ô∏è Ch∆∞a c√≥ Site Profile! D√πng search m·∫∑c ƒë·ªãnh.")
                self.logger.warning("   H√£y t·∫°o profile trong Dashboard > Settings > Site Profile")
                search_query = self.keyword
                strategy = {}
            else:
                # Classify keyword
                keyword_type = adaptive_system.classify_keyword(self.keyword, ai_client)
                self.logger.info(f"üìä Keyword type: {keyword_type}")
                
                # Get strategy
                strategy = adaptive_system.get_search_strategy(self.keyword, keyword_type)
                
                # Build optimized query
                search_query = adaptive_system.build_search_query(self.keyword, strategy)
                self.logger.info(f"üîç Search query: {search_query}")
        except Exception as e:
            self.logger.warning(f"Adaptive search error: {e}, using default")
            search_query = self.keyword
            strategy = {}
            adaptive_system = None
        
        # G·ªçi Google Custom Search API
        self.logger.info(f"ƒêang t√¨m ki·∫øm Google cho: {search_query}")
        
        search_url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'key': api_key,
            'cx': cse_id,
            'q': search_query,  # Use optimized query
            'num': 10,  # L·∫•y 10 k·∫øt qu·∫£ ƒë·ªÉ c√≥ nhi·ªÅu l·ª±a ch·ªçn
            'lr': 'lang_vi',  # ∆Øu ti√™n ti·∫øng Vi·ªát
        }
        
        try:
            response = requests.get(search_url, params=params, timeout=15)
            data = response.json()
            
            # Debug: In ra response ƒë·ªÉ ki·ªÉm tra
            if 'error' in data:
                self.logger.error(f"Google API Error: {data['error'].get('message', data['error'])}")
                return
            
            if 'items' not in data or len(data['items']) == 0:
                self.logger.error(f"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ cho: {self.keyword}")
                self.logger.info(f"Response t·ª´ Google: {data.get('searchInformation', {})}")
                return
            
            # L·∫•y URL ph√π h·ª£p nh·∫•t v·ªõi adaptive filtering
            target_url = None
            target_image = None
            target_title = None
            
            # Get domains from strategy (if available)
            skip_domains = [
                'youtube.com', 'facebook.com', 'tiktok.com', 
                'twitter.com', 'instagram.com', 'pinterest.com',
                'amazon.com', 'shopee.vn', 'lazada.vn'
            ]
            
            # Priority domains from adaptive system or fallback
            priority_domains = strategy.get('priority_domains', []) if strategy else []
            if not priority_domains:
                # Fallback to default
                priority_domains = [
                    'truyenfull', 'truyen', 'metruyencv', 'tangthuvien',
                    'wikidich', 'sstruyen', 'truyenyy', 'novelhall'
                ]
            
            # Store candidates with scores
            candidates = []
            
            for item in data['items']:
                url = item.get('link', '')
                title = item.get('title', '')
                snippet = item.get('snippet', '')
                domain = urlparse(url).netloc.lower()
                
                # B·ªè qua c√°c trang kh√¥ng mong mu·ªën
                if any(skip in domain for skip in skip_domains):
                    continue
                
                # Score relevance if adaptive system available
                if adaptive_system and strategy:
                    relevance_score = adaptive_system.score_result_relevance(title, snippet, strategy)
                    
                    if relevance_score < 0.3:
                        self.logger.info(f"‚è≠Ô∏è Skipped (low relevance {relevance_score:.2f}): {title[:50]}...")
                        continue
                else:
                    relevance_score = 0.5  # Default score
                
                # Check priority domain
                is_priority = any(p in domain for p in priority_domains)
                
                # L·∫•y h√¨nh ·∫£nh t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm (n·∫øu c√≥)
                pagemap = item.get('pagemap', {})
                
                # Th·ª≠ l·∫•y ·∫£nh t·ª´ nhi·ªÅu ngu·ªìn
                image_url = None
                
                # 1. T·ª´ cse_image
                if 'cse_image' in pagemap:
                    image_url = pagemap['cse_image'][0].get('src', '')
                
                # 2. T·ª´ cse_thumbnail
                if not image_url and 'cse_thumbnail' in pagemap:
                    image_url = pagemap['cse_thumbnail'][0].get('src', '')
                
                # 3. T·ª´ metatags og:image
                if not image_url and 'metatags' in pagemap:
                    for meta in pagemap['metatags']:
                        if 'og:image' in meta:
                            image_url = meta['og:image']
                            break
                
                # Add to candidates
                candidates.append({
                    'url': url,
                    'title': title,
                    'image': image_url,
                    'relevance': relevance_score,
                    'is_priority': is_priority
                })
            
            # Sort candidates by priority and relevance
            if candidates:
                candidates.sort(key=lambda x: (x['is_priority'], x['relevance']), reverse=True)
                
                # Pick best candidate
                best = candidates[0]
                target_url = best['url']
                target_image = best['image']
                target_title = best['title']
                
                self.logger.info(f"‚úÖ Best result (score: {best['relevance']:.2f}): {target_title[:60]}...")
                
                # Learn from this search
                if adaptive_system and strategy:
                    try:
                        should_refine = adaptive_system.learn_from_search(
                            self.keyword,
                            keyword_type if 'keyword_type' in locals() else 'default',
                            search_query,
                            target_url,
                            target_title
                        )
                        
                        if should_refine:
                            self.logger.info("üîÑ Auto-refining profile...")
                            adaptive_system.refine_profile(ai_client)
                            self.logger.info("‚úÖ Profile refined!")
                    except Exception as e:
                        self.logger.warning(f"Learning error: {e}")
            
            if target_url:
                self.logger.info(f"‚úÖ T√¨m th·∫•y URL: {target_url}")
                if target_image:
                    self.logger.info(f"‚úÖ T√¨m th·∫•y ·∫£nh t·ª´ Google: {target_image}")
                
                yield scrapy.Request(
                    url=target_url,
                    callback=self.parse,
                    errback=self.handle_error,  # X·ª≠ l√Ω l·ªói
                    meta={
                        'keyword': self.keyword,
                        'google_image': target_image,
                        'dont_retry': False,
                    },
                    dont_filter=True
                )
            else:
                self.logger.error("Kh√¥ng t√¨m th·∫•y URL ph√π h·ª£p!")
                
        except Exception as e:
            self.logger.error(f"L·ªói khi g·ªçi Google API: {e}")

    def handle_error(self, failure):
        """X·ª≠ l√Ω l·ªói khi request th·∫•t b·∫°i"""
        request = failure.request
        self.logger.error(f"‚ùå Request th·∫•t b·∫°i: {request.url}")
        self.logger.error(f"   L·ªói: {failure.value}")
        
        # Log chi ti·∫øt h∆°n
        if failure.check(twisted_error.ConnectionRefusedError):
            self.logger.error("   ‚Üí Website t·ª´ ch·ªëi k·∫øt n·ªëi (c√≥ th·ªÉ b·ªã ch·∫∑n IP)")
        elif failure.check(twisted_error.TimeoutError):
            self.logger.error("   ‚Üí Timeout - website ph·∫£n h·ªìi qu√° ch·∫≠m")
        elif failure.check(twisted_error.DNSLookupError):
            self.logger.error("   ‚Üí Kh√¥ng th·ªÉ ph√¢n gi·∫£i DNS - website c√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i")

    def parse(self, response):
        """
        Tr√≠ch xu·∫•t n·ªôi dung v√† h√¨nh ·∫£nh t·ª´ trang web
        """
        self.logger.info(f"ƒêang parse: {response.url}")
        
        # === 1. TR√çCH XU·∫§T N·ªòI DUNG ===
        # D√πng Trafilatura ƒë·ªÉ l·ªçc n·ªôi dung s·∫°ch
        downloaded = trafilatura.fetch_url(response.url)
        clean_text = trafilatura.extract(
            downloaded, 
            include_comments=False, 
            include_tables=True,
            include_images=False
        )
        
        # Fallback n·∫øu trafilatura th·∫•t b·∫°i
        if not clean_text or len(clean_text) < 200:
            self.logger.warning("Trafilatura kh√¥ng l·∫•y ƒë∆∞·ª£c, d√πng Scrapy...")
            
            # Th·ª≠ nhi·ªÅu selector kh√°c nhau
            content_selectors = [
                'article p::text',
                '.post-content p::text',
                '.entry-content p::text',
                '.content p::text',
                '#content p::text',
                '.chapter-content::text',
                '.chapter-c::text',
                'p::text'
            ]
            
            paragraphs = []
            for selector in content_selectors:
                paragraphs = response.css(selector).getall()
                if paragraphs and len(' '.join(paragraphs)) > 200:
                    break
            
            clean_text = ' '.join(paragraphs)
        
        if not clean_text or len(clean_text) < 100:
            self.logger.error("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung t·ª´ trang!")
            return

        # === 2. TR√çCH XU·∫§T H√åNH ·∫¢NH ===
        image_url = ""
        
        # ∆Øu ti√™n 1: ·∫¢nh t·ª´ Google Search (ƒë√£ l·∫•y ·ªü b∆∞·ªõc tr∆∞·ªõc)
        google_image = response.meta.get('google_image')
        if google_image and google_image.startswith('http'):
            image_url = google_image
            self.logger.info(f"D√πng ·∫£nh t·ª´ Google Search: {image_url}")
        
        # ∆Øu ti√™n 2: Open Graph image (th∆∞·ªùng l√† ·∫£nh ƒë·∫°i di·ªán ch·∫•t l∆∞·ª£ng)
        if not image_url:
            og_image = response.css('meta[property="og:image"]::attr(content)').get()
            if og_image:
                image_url = self._normalize_url(og_image, response.url)
                self.logger.info(f"D√πng og:image: {image_url}")
        
        # ∆Øu ti√™n 3: Twitter card image
        if not image_url:
            twitter_image = response.css('meta[name="twitter:image"]::attr(content)').get()
            if twitter_image:
                image_url = self._normalize_url(twitter_image, response.url)
                self.logger.info(f"D√πng twitter:image: {image_url}")
        
        # ∆Øu ti√™n 4: ·∫¢nh trong article/post
        if not image_url:
            img_selectors = [
                'article img::attr(src)',
                '.post-thumbnail img::attr(src)',
                '.featured-image img::attr(src)',
                '.entry-content img::attr(src)',
                '.post-content img::attr(src)',
                '.wp-post-image::attr(src)',
                '.book-cover img::attr(src)',
                '.cover img::attr(src)',
            ]
            
            for selector in img_selectors:
                img = response.css(selector).get()
                if img:
                    # L·ªçc b·ªè c√°c ·∫£nh icon, avatar nh·ªè
                    if self._is_valid_image(img):
                        image_url = self._normalize_url(img, response.url)
                        self.logger.info(f"D√πng ·∫£nh t·ª´ selector '{selector}': {image_url}")
                        break
        
        # ∆Øu ti√™n 5: ·∫¢nh ƒë·∫ßu ti√™n trong trang (fallback)
        if not image_url:
            all_images = response.css('img::attr(src)').getall()
            for img in all_images:
                if self._is_valid_image(img):
                    image_url = self._normalize_url(img, response.url)
                    self.logger.info(f"D√πng ·∫£nh fallback: {image_url}")
                    break
        
        if not image_url:
            self.logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh ph√π h·ª£p")

        # === 3. T·∫†O ITEM ===
        item = BlogPostItem()
        item['keyword'] = response.meta['keyword']
        item['source_url'] = response.url
        item['raw_text'] = clean_text
        item['image_url'] = image_url
        
        self.logger.info(f"‚úÖ ƒê√£ l·∫•y {len(clean_text)} k√Ω t·ª± n·ªôi dung")
        if image_url:
            self.logger.info(f"‚úÖ ƒê√£ l·∫•y h√¨nh ·∫£nh: {image_url[:80]}...")
        
        yield item
    
    def _normalize_url(self, url, base_url):
        """Chuy·ªÉn URL t∆∞∆°ng ƒë·ªëi th√†nh URL tuy·ªát ƒë·ªëi"""
        if not url:
            return ""
        
        url = url.strip()
        
        # ƒê√£ l√† URL ƒë·∫ßy ƒë·ªß
        if url.startswith('http://') or url.startswith('https://'):
            return url
        
        # URL protocol-relative
        if url.startswith('//'):
            return 'https:' + url
        
        # URL t∆∞∆°ng ƒë·ªëi
        return urljoin(base_url, url)
    
    def _is_valid_image(self, url):
        """Ki·ªÉm tra xem URL c√≥ ph·∫£i l√† ·∫£nh h·ª£p l·ªá kh√¥ng"""
        if not url:
            return False
        
        url_lower = url.lower()
        
        # B·ªè qua c√°c ·∫£nh nh·ªè, icon
        skip_patterns = [
            'avatar', 'icon', 'logo', 'favicon', 'emoji',
            'button', 'banner-ad', 'advertisement', 'ads',
            '1x1', '16x16', '32x32', '64x64',
            'pixel', 'tracking', 'blank', 'spacer',
            'loading', 'spinner', 'placeholder'
        ]
        
        if any(pattern in url_lower for pattern in skip_patterns):
            return False
        
        # Ph·∫£i l√† ƒë·ªãnh d·∫°ng ·∫£nh
        valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        has_valid_ext = any(ext in url_lower for ext in valid_extensions)
        
        # Ho·∫∑c l√† URL kh√¥ng c√≥ extension (c√≥ th·ªÉ l√† ·∫£nh ƒë·ªông)
        if not has_valid_ext:
            # Ki·ªÉm tra xem c√≥ ph·∫£i URL ·∫£nh t·ª´ CDN kh√¥ng
            cdn_patterns = ['images', 'img', 'photo', 'pic', 'media', 'uploads', 'wp-content']
            if not any(pattern in url_lower for pattern in cdn_patterns):
                return False
        
        return True